---
layout: default
---

This group is interested in NLP applications with a particular focus on parsing and machine translation.


<iframe src="https://calendar.google.com/calendar/embed?mode=AGENDA&amp;height=250&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=iuesktj5bg3jmil7kjjtpplju4%40group.calendar.google.com&amp;color=%23853104&amp;ctz=Europe%2FAmsterdam" style="border-width:0" width="600" height="250" frameborder="0" scrolling="no"></iframe>

Scheduled:

* Probabilistic Matrix Factorization

Done:

* Dynamic Programming for Linear-Time Incremental Parsing
* Trainable Greedy Decoding for Neural Machine Translation
* Fully Character-Level Neural Machine Translation without Explicit Segmentation
* Neural Machine Translation in Linear Time
* A Convolutional Encoder Model for Neural Machine Translation
* Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning
* A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
* The Neural Noisy Channel
* What Do Recurrent Neural Network Grammars Learn About Syntax?
* Recurrent Neural Network Grammars
* Tree-To-Sequence Attentional Neural Machine Translation
* LSTM: A Search Space Odyssey
* Nonparametric Spherical Topic Modeling with Word Embeddings
* Word Embeddings as Metric Recovery in Semantic Spaces
* Variational Neural Machine Translation
* Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding
* Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations
* Incremental Parsing with Minimal Features Using Bi-Directional LSTM
* Most "babies" are "little" and most "problems" are "huge": Compositional Entailment in Adjective-Nouns
