---
layout: default
title: Deep Learning for NLP
menu: no
---

We also read about deep learning (even when latent variables are nowhere to be seen).

# Scheduled

We moved our activities to [this page](//cl-illc.github.io/content/dl).

# Pool

* [Unsupervised Learning of Task-Specific Tree Structures with Tree-LSTMs](https://arxiv.org/pdf/1707.02786.pdf)
* [Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs](https://arxiv.org/pdf/1705.09189.pdf)
* [Grammar variational auto-encoder](https://arxiv.org/abs/1703.01925)
* [Pointer Networks](https://arxiv.org/pdf/1506.03134.pdf)
* [Key-Value Memory Networks for Directly Reading Documents](https://arxiv.org/pdf/1606.03126.pdf)

# Done

* June 30: [Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation](https://arxiv.org/pdf/1308.3432.pdf) and Gumbel relaxations ([a](https://arxiv.org/abs/1611.00712) and [b](https://arxiv.org/pdf/1611.01144.pdf))
* June 15: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
* June 9: [Learning Structured Text Representations](https://arxiv.org/pdf/1705.09207.pdf) and [Structured Attention Networks](https://arxiv.org/pdf/1702.00887.pdf)
* May 17: [Frustratingly Short Attention Spans in Neural Language Modeling](https://arxiv.org/pdf/1702.04521.pdf)
