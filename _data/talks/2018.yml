-
  layout: talk
  selected: y
  year: 2018
  event: ACL18 Tutorial, Melbourne, Australia.
  img: vi-tutorial
  title: Variational Inference and Deep Generative Models
  authors: Wilker Aziz and Philip Schulz
  doc-url: https://github.com/philschulz/VITutorial
  slides: https://github.com/philschulz/VITutorial
  abstract: >
      NLP has seen a surge in neural network models in recent years. These models provide state-of-the-art performance on many supervised tasks. Unsupervised and semi-supervised learning has only been addressed scarcely, however. Deep Generative Models (DGMs) make it possible to integrate neural networks with probabilistic graphical models. Using DGMs one can easily design latent variable models that account for missing observations and thereby enable unsupervised and semi-supervised learning with neural networks. The method of choice for training these models is variational inference. This tutorial offers a general introduction to variational inference followed by a thorough and example-driven discussion of how to use variational methods for training DGMs. It provides both the mathematical background necessary for deriving the learning algorithms as well as practical implementation guidelines. Moreover, we discuss common pitfalls that one may encounter when using DGMs for NLP applications, such as the latent variable being ignored by the model, and discuss potential solutions from a theoretical and practical perspective. Importantly, the tutorial will cover models with continuous and discrete variables.
  about: |
      Tutorial on variational inference and deep generative models.
  schedule: |
    This tutorial is scheduled to be presented at

    * ACL 2018 in Melbourne: July 2018
    * University of Amsterdam: March 22-23 2018
    
    and it has already been presented at

    * Monash University on November 16 2017
    * University of Melbourne on October 31 and November 2 2017
    * Amazon (Berlin) on July 26-27 2017
-
  layout: talk
  selected: y
  year: 2018
  event: 03/04/2018, 11pm, Naver labs (France)
  img: dgm-naver
  title: Probabilistic modelling for NLP powered by deep learning
  authors: Wilker Aziz
  doc-url: /slides/dgms-at-naver-2018.pdf
  slides: /slides/dgms-at-naver-2018.pdf
  abstract: >
    "Deep generative models (DGMs) are probabilistic models parametrised by neural networks (NNs). DGMs combine the power of NNs with the generality of the probabilistic learning framework allowing a modeller to be more explicit about her statistical assumptions. To unlock this power however one must consider efficient ways to approach probabilistic inference. Variational inference surfaced as the method of choice, howerver, efficient and effective VI for DGMs require low-variance gradient estimation for stochastic computation graphs (Kingma and Welling, 2013; Rezende et al, 2014; Titsias and Lazaro-Gredilla, 2014). In this talk I will present an overview of deep generative modelling, amortised variational inference, and the mathematics behind low-variance reparameterised gradients."
  about: > 
    Invited talk at Naver Labs (France). The original talk was divide into two session and I also presented models that I have developed with my students and collaborators.

